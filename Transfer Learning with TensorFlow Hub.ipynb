{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Getting Started with Transfer Learning Using Tensorflow Hub**"},{"metadata":{},"cell_type":"markdown","source":"Hi Kagglers! Welcome to the introductory notebook on using **TensorFlow Hub** for building transfer learning models. This notebook is focused on quickly get you started with building sophisticated models with very little knowledge of modeling with **TensorFlow**. Let's get started!\n\n** Please upvote, if you find this notebook useful in any way!**"},{"metadata":{},"cell_type":"markdown","source":"### What is Transfer Learning?\n**Transfer Learning** is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem (Source: [Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)). \n\nTransfer Learning overcomes the problem of isolated learning by applying the knowledge gained during learning one task to another different but similar task. For example, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks! The intution actually came from the humans. We humans always practice *Transfer Learning*. A person who knows how to drive a bike, finds it easy to learn how to drive a car. Similarly *Transfer Learning* helps in learning difficult tasks with less effort."},{"metadata":{},"cell_type":"markdown","source":"### How Does TensorFlow Hub Help Us?\n**TensorFlow Hub** is a repository of pre-trained machine learning models. These models are categorised in three broad problem domains -\n+ Image\n+ Text\n+ Video\n\nThere are several models, which you can quickly start using without much hassle. Visit [TensorFlow Hub](https://tfhub.dev/) for more details."},{"metadata":{},"cell_type":"markdown","source":"### The Problem\nFor the demonstration of the transfer learning technique, I will be using the case of [](http://)[this](https://www.kaggle.com/c/nlp-getting-started) competition. The goal here is to build a machine learning model which is able to predict whether a tweet belongs to a real disaster or not!"},{"metadata":{},"cell_type":"markdown","source":"### Dataset\nDataset given here is tweets from different users split into training and test set. **target** is the dependent varible, which we are interested in predicting for the test set. Let's begin with our analysis."},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries"},{"metadata":{"id":"8uU844tqWC91","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# General Purpose Libraries\nimport numpy as np\nnp.random.seed(1)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\n# Text Processing Libraries\nimport spacy\nimport re\nimport string\n\n# Scikit Learn\nfrom sklearn.model_selection import train_test_split\n\n# TensorFlow\nimport tensorflow_hub as hub\nimport tensorflow as tf\ntf.random.set_seed(1)\n\n# Setting Pandas Display Option\npd.set_option('display.max_colwidth', 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data\nLet's begin by loading the dataset in our environment and then take a peek at the dataset."},{"metadata":{"id":"zGIX7cVP-xJ9","trusted":true},"cell_type":"code","source":"# Reading Data Files\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nprint(\"Train Shape :\", train.shape)\nprint(\"Test Shape :\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"AruGuFus-_Ks","outputId":"0c2788de-f2c4-4b43-f08c-968f6897460b","trusted":true},"cell_type":"code","source":"# Viewing top rows of training set\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing top rows of test set\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"+ There are *7613* rows and *5* columns in the training set.\n+ There are *3263* rows and *4* columns in the test set.\n+ *target* is the dependent variable, with values *0* and *1* indicating a fake and real disaster tweet respectively.\n+ *id* is just an unique identifier of the tweet. Training and Test set are subsets of a sample of tweets collected.\n+ There are many missing values in *location* variable."},{"metadata":{},"cell_type":"markdown","source":"## Basic EDA\nLet's do some basic EDA to better understand the dataset provided."},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*location* has many missing values. Are those missing values indicate non-legitimate tweets? There might be some information here. We will look at it at a later stage."},{"metadata":{},"cell_type":"markdown","source":"### The Dependent Variable - *target*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Classes\ntrain['target'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Independent Variables\n#### *keyword*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unique Words Count\ntrain.keyword.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are *221* unique words present in training set. Some text cleaning is required here to extract some information from this variable."},{"metadata":{},"cell_type":"markdown","source":"#### *location*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unique Words Count\ntrain.location.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many repetitions of location in the dataset with multiple names. For example - USA, United States & New York are separate entries. Also there are some numbers in this variable. We need to address these issues before making use to this variable in our model."},{"metadata":{},"cell_type":"markdown","source":"#### *text*\nThis is our main variable which contains actual tweets from different users. Let's take a look at some random tweets to get a sense of how the data is and what kind of pre-processing is required."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As one can expect from any textual data, there are all sorts of messyness going on here. There are numbers, special characters, links, punctuation marks etc. present in the tweets. We need to clean these before we proceed with modeling, in order to get good results."},{"metadata":{},"cell_type":"markdown","source":"## Text Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Text Cleaning\nLets begin our text preprocessing by creating a custom function to remove numbers, links, punctuations etc. The following function has been taken from [Parul Pandey](https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro)'s notebook. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom Function for Text Cleaning\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the cleaning function to both test and training datasets\ntrain['cleaned_text'] = train['text'].apply(lambda x: clean_text(x))\ntest['cleaned_text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Take a look at the cleaned text\ntrain['cleaned_text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization\n*Lemmatization* is a technique where a word is reduced to its base or dictionary form. Like - *Go*, *Going* & *Gone* will all be replaced by *Go*. Let's create a custom function to lemmatize our tweets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import spaCy's language model\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# function to lemmatize text\ndef lemmatization(texts):\n    output = []\n    for i in texts:\n        s = [token.lemma_ for token in nlp(i)]\n        output.append(' '.join(s))\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the lemmatization function to both test and training datasets\ntrain['cleaned_text'] = lemmatization(train['cleaned_text'])\ntest['cleaned_text'] = lemmatization(test['cleaned_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the cleaned & lemmatized text\ntrain.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling Using Pretrained Model"},{"metadata":{},"cell_type":"markdown","source":"I will be using [Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4) for modeling. It encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.\n\nThe model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. There are two variations of this model on *TensorFlow Hub* -\n+ Deep Averaging Network (DAN) Encoder.\n+ Transformer Encoder.\n\nI will be using the DAN one, since it is computationally less intensive. We will download this model as a *KearsLayer* and use it as input layer for our Keras Sequential Model."},{"metadata":{"id":"z1PkdZVzmFjH","trusted":true},"cell_type":"code","source":"# Splitting training & testing set\nx_train, x_test, y_train, y_test = train_test_split(train['cleaned_text'], train['target'], \n                                                    test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a Keras Sequential Model using Pre-trained Universal Sentence Encoder as the Input Layer\nhub_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4', \n                        input_shape = [],\n                        output_shape = [512],\n                        dtype = tf.string, \n                        trainable = True)\n\nmodel = tf.keras.models.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(1, activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"7sRfdiBlk8k2","trusted":true},"cell_type":"code","source":"# Complile the Model\nmodel.compile(optimizer = 'adam', \n              loss = 'binary_crossentropy', \n              metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"p54muqXSl-sA","outputId":"b8bf119c-f79d-4c5b-d791-749a413d44cd","trusted":true},"cell_type":"code","source":"# Train the Model\nmodel.fit(x_train, \n          y_train, \n          epochs = 1, \n          validation_data = (x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction\nLet's test our model on some random text."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing on random text\nmodel.predict([\"I am feeling robbed!\", \"there is a robbery going on, please help!\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our model is doing a fairly good job (even with raw text, without any preprocessing) in differentiating the context of both sentences, which is great! I am sure you can come-up with better text to test it :-).\nLet's use this model to predict on our test set now and see how it performs."},{"metadata":{"id":"GTmSWIyQTfsy","outputId":"f2bf60c2-1a40-4770-98d4-4c7699040c30","trusted":true},"cell_type":"code","source":"# Predict on Test Set\npred = model.predict(test['cleaned_text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare for Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Submission File and \nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = np.round(pred).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"89RwZh2-V6C0","outputId":"27dfb4b3-7758-4cad-b7da-d104f1fa9a4b","trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thanks for taking out time to go through my notebook! Go ahead and experiment with other *TensorFlow Hub* models and try to improve the results.**"}],"metadata":{"colab":{"name":"NLP - Deep Learning","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}